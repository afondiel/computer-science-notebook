{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open notebook in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/afondiel/computer-science-notebook/tree/master/core/ai-ml/deep-learning-notes/neural-nets/notebook/attention_is_all_you_need.ipynb)"
      ],
      "metadata": {
        "id": "EPAPjUs-r27i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7ZIhXbxPDbS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img width=\"600\" height=\"300\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6438a9027de34e8ea7e4b257/4MMtJDefZBU8dpmHana6B.png\">\n",
        "\n",
        "\n",
        "Source: https://huggingface.co/blog/Jaward/coding-your-first-attention"
      ],
      "metadata": {
        "id": "l-IzNS06ip7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def attention(Q, K, V):\n",
        "  dk = Q.size(-1)\n",
        "  scores = torch.matmul(Q, K.transpose(-2, -1))/np.sqrt(dk)\n",
        "  if mask is not None:\n",
        "    scores = scores.masked_fill(mask == 0, -1e9)\n",
        "  p_attn = F.softmax(scores, dim=-1)\n",
        "  out = torch.matmul(p_attn, V)\n",
        "  return out\n"
      ],
      "metadata": {
        "id": "ZAvlxnKUi6Th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: write a code to test this function: attention(Q, K, V)\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# sample input tensors\n",
        "Q = torch.randn(2, 3, 4)\n",
        "K = torch.randn(2, 3, 4)\n",
        "V = torch.randn(2, 3, 4)\n",
        "mask = None\n",
        "\n",
        "# call the attention function\n",
        "output = attention(Q, K, V)\n",
        "\n",
        "# print the output\n",
        "print(f\"Q: {Q}, \\nK: {K}, \\nV: {V}\")\n",
        "print(f\"Q-shape: {Q.shape}, \\nK-shape: {K.shape}, \\nV-shape: {V.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEvsvrcaqlKd",
        "outputId": "449de714-a2f4-48a4-a2b1-6055e0e90b02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: tensor([[[ 0.3248, -0.2816,  0.5649, -0.5883],\n",
            "         [ 0.1014, -0.2892, -1.1494, -0.0343],\n",
            "         [ 1.1611, -1.1778, -0.4965, -2.5952]],\n",
            "\n",
            "        [[-0.8530, -0.4816,  0.1375,  1.1548],\n",
            "         [-1.0562, -0.5972, -2.0863, -0.6770],\n",
            "         [ 0.1306, -0.0780, -0.2148, -1.5332]]]), \n",
            "K: tensor([[[-0.6672,  0.3029, -1.2540, -0.2968],\n",
            "         [-0.1970, -1.3694,  0.1549,  0.1588],\n",
            "         [ 0.2104,  0.0468,  0.0048, -0.3037]],\n",
            "\n",
            "        [[-0.0449, -0.5207, -0.1919, -1.0864],\n",
            "         [ 0.0602, -0.0561,  0.7803, -0.5877],\n",
            "         [ 0.1192,  1.6522, -0.0620,  1.8709]]]), \n",
            "V: tensor([[[-1.4642,  1.5672, -0.4723,  1.6154],\n",
            "         [-0.7321,  1.4285, -0.7772, -2.8687],\n",
            "         [ 0.2703, -0.6095, -0.6333, -2.0936]],\n",
            "\n",
            "        [[ 0.0725, -0.3944,  0.8649,  0.9405],\n",
            "         [-0.2680, -0.1354,  0.0665,  0.0850],\n",
            "         [-0.6947,  0.6490,  0.2722,  1.5652]]])\n",
            "Q-shape: torch.Size([2, 3, 4]), \n",
            "K-shape: torch.Size([2, 3, 4]), \n",
            "V-shape: torch.Size([2, 3, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"output: {output}, output-shape: {output.shape}\")"
      ],
      "metadata": {
        "id": "J9lOLXPwtMRH",
        "outputId": "f2f2eb30-c280-48f3-d94a-3aa98049363d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output: tensor([[[-0.5134,  0.6832, -0.6544, -1.5743],\n",
            "         [-0.8294,  0.9842, -0.5961, -0.5416],\n",
            "         [-0.5480,  0.6991, -0.6429, -1.3978]],\n",
            "\n",
            "        [[-0.4515,  0.2713,  0.3368,  1.1063],\n",
            "         [-0.0725, -0.2338,  0.6568,  0.8553],\n",
            "         [-0.0918, -0.2438,  0.5469,  0.6699]]]), output-shape: torch.Size([2, 3, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img width=\"600\" height=\"800\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6438a9027de34e8ea7e4b257/c-RzcFcoyRVFqYCSxgvsS.png\">\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TUrjixB5mPoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Scaled Dot-Product Attention // Self-Attention\n",
        "class SingleHeadAttention(nn.Module):\n",
        "    def __init__(self, in_dim, attn_dim):\n",
        "        super(SingleHeadAttention, self).__init__()\n",
        "        self.Q_linear = nn.Linear(in_dim, attn_dim)\n",
        "        self.K_linear = nn.Linear(in_dim, attn_dim)\n",
        "        self.V_linear = nn.Linear(in_dim, attn_dim)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None, dropout=None):\n",
        "        Q_proj = self.Q_linear(Q)\n",
        "        K_proj = self.K_linear(K)\n",
        "        V_proj = self.V_linear(V)\n",
        "\n",
        "        dk = Q.size(-1)\n",
        "\n",
        "        scores = torch.matmul(Q_proj, K_proj.transpose(-2, -1)) / math.sqrt(dk)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        p_attn = F.softmax(scores, dim=-1)\n",
        "\n",
        "        if dropout is not None:\n",
        "            p_attn = dropout(p_attn)\n",
        "\n",
        "        out = torch.matmul(p_attn, V_proj)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "L_I4OrZ8AVOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test for SingleHeadAttention\n",
        "single_head_attn = SingleHeadAttention(in_dim=512, attn_dim=64)\n",
        "Q_test = torch.randn(2, 10, 512)\n",
        "K_test = torch.randn(2, 10, 512)\n",
        "V_test = torch.randn(2, 10, 512)\n",
        "output_single = single_head_attn(Q_test, K_test, V_test)\n",
        "print(\"Single Head Attention Output Shape:\", output_single.shape)  # Expected: [2, 10, 64]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6deCbu6DrWt",
        "outputId": "fffef15b-aadf-4056-a63c-85bc9dbdbcd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single Head Attention Output Shape: torch.Size([2, 10, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img width=\"600\" height=\"800\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6438a9027de34e8ea7e4b257/QfUeWOfSU1J64OR7yIYUn.png\">"
      ],
      "metadata": {
        "id": "T3hMWmIcmPcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-Head Attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, in_dim, attn_dim, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.heads = nn.ModuleList(\n",
        "            SingleHeadAttention(in_dim, attn_dim) for _ in range(num_heads)\n",
        "        )\n",
        "        self.linear = nn.Linear(num_heads * attn_dim, in_dim)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None, dropout=None):\n",
        "        head_outputs = [head(Q, K, V, mask, dropout) for head in self.heads]\n",
        "        concatenated_outputs = torch.cat(head_outputs, dim=-1)\n",
        "        output = self.linear(concatenated_outputs)\n",
        "        return output"
      ],
      "metadata": {
        "id": "vMGdBCGXmOzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test for MultiHeadAttention\n",
        "multi_head_attn = MultiHeadAttention(in_dim=512, attn_dim=64, num_heads=8)\n",
        "output_multi = multi_head_attn(Q_test, K_test, V_test)\n",
        "print(\"Multi-Head Attention Output Shape:\", output_multi.shape)  # Expected: [2, 10, 512]\n"
      ],
      "metadata": {
        "id": "y7gZe1vYC6Oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "- [Let's build GPT: from scratch, in code, spelled out. - 37:46](https://www.youtube.com/watch?v=kCc8FmEb1nY)\n",
        "\n",
        "HuggingFace:\n",
        "\n",
        "- [On Coding Your First Attention - HF](https://huggingface.co/blog/Jaward/coding-your-first-attention)\n",
        "- [Attention Is All You Need (paper overview)](https://huggingface.co/papers/1706.03762)\n",
        "\n",
        "Google:\n",
        "- [Attention Mechanism](https://www.youtube.com/watch?v=fjJOgb-E41w&list=PLIivdWyY5sqIlLF9JHbyiqzZbib9pFt4x&index=3)\n",
        "- [Attention Mechanism: Overview](https://www.youtube.com/watch?v=8PmOaVYVeKY&list=PLBgogxgQVM9s0i9oloJwjIG-zj6Svsm20&index=2)\n",
        "\n",
        "Transformer Notes:\n",
        "- https://docs.google.com/document/d/19zFJ4qWq7u3x5sKCd3ej0v9z9Oozo08g_rRPuqTnYls/edit\n",
        "- https://github.com/afondiel/computer-science-notes/blob/master/ai/nlp-notes/models/transformers-notes.md"
      ],
      "metadata": {
        "id": "ef6GoBCPPOCE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iZbl9KFmPyzH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}